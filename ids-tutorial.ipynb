{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a232eb3a",
   "metadata": {},
   "source": [
    "# Building a network-based intrusion detection system for consumer electronics: From raw data to deployment\n",
    "\n",
    "---\n",
    "This is a tutorial that was presented at **IEEE 44th International Conference on Consumer Electronics** held in Dubai, UAE. Feb 3-5, 2026.\n",
    "\n",
    "Prepared and presented by **[Prof. Mohammed M. Alani](https://www.rit.edu/directory/moacad-mohammed-m-al-ani)**, Rochester Institute of Tehcnology- Dubai, UAE.\n",
    "\n",
    "Abstract of the tutorial can be found [here](https://icce.org/2026/tutorials/).\n",
    "\n",
    "Complete github repo can be found [here](https://github.com/Mo-Alani/icce26-ids-tutorial)\n",
    "\n",
    "---\n",
    "\n",
    "## Some recommended prerequisites\n",
    "Here are some recommendations before you dive in.\n",
    "\n",
    "1. Install Python using [Mini Conda](https://www.anaconda.com/docs/getting-started/miniconda/main), and create a virtual environment with the latest version of Python (3.13.11 at the time of publishing this). \n",
    "\n",
    "2. My recommendation is to use Linux to easily install the needed tools. Install your choice of editor. Mine is VSCodium, with Python, and Jupyter plugins. Start your favorite music that keeps you in the coding mood.\n",
    "\n",
    "3. Install wireshark, tshark for feature extraction. (Or zeek?!)\n",
    "\n",
    "4. Install the following Python libraries:\n",
    "    * pandas\n",
    "    * numpy\n",
    "    * scikit-learn\n",
    "    * xgboost\n",
    "    * imblearn\n",
    "    * tqdm\n",
    "    * matplotlib\n",
    "\n",
    "*PS:* You'll see that I'm re-importing libraries in different cells of this notebook. The reason is that I wanted to make each cell independent from the previous ones so you can copy-and-paste them into individual files, if Jupyter notebooks is not your preferred method of coding.\n",
    "\n",
    "## The dataset and feature extraction\n",
    "The dataset to be used in this tutorial can be downloaded from IEEE DataPort:\n",
    "\n",
    "<https://dx.doi.org/10.21227/q70p-q449>\n",
    "\n",
    "The complete dataset includes two parts; the raw pcap files along with an Excel sheet that contains the truth table for the labelling of the dataset.\n",
    "The features can be extracted at a packet-level, or at the network flow level. The choice is yours.\n",
    "\n",
    "1. **Network flow-based** feature extraction is quite common in intrusion detection systems, and can be done using tools such as [Zeek](https://zeek.org/), or using [this](https://github.com/ahlashkari/CICFlowMeter) great tool by Arash Lashkari.\n",
    "\n",
    "2. **Packet-based** feature extraction can be more effective for some types of short attacks (such as port scanning), and can be performed with tools like [tshark](https://www.wireshark.org/docs/man-pages/tshark.html). \n",
    "\n",
    "We have done some work that combines both that can be found [here](https://doi.org/10.1109/TII.2022.3192035).\n",
    "\n",
    "Our choice for this tutorial is packet-based features. The features can be extracted using the tshark command:\n",
    "\n",
    "`tshark -r YOUR_PCAP_FILE.pcap -T fields -e FIELD_NAME -E separator=, -E qoute=d`\n",
    "\n",
    "Replace the file name with the pcap fie of your choice, and add as many fields (using `-e FIELD_NAME`)as you see suitable for your experiment. The fields are quite similar to the filters that can be used in Wireshark. You can find a complete list [here](https://www.wireshark.org/docs/dfref/). Due to the lack of time, we won't be able to dive into this in any more details.\n",
    "\n",
    "For this experiment, we combined a few pcap files that are focused on botnet traffic using Wireshark, passed them through `tshark` to extract the features in csv format. Then, we labelled the traffic based on the truth table file, with 0 being a benign packet, and 1 being a malicious packet. We took care of the tons of empty fields as explained in the tutorial session. Finally, we created the file named: dataset1.csv. Make sure to unzip the file before the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's generate some information about the dataset\n",
    "from pandas import read_csv\n",
    "from collections import Counter\n",
    "\n",
    "url = \"dataset1.csv\"\n",
    "print('Dataset file:', url)\n",
    "\n",
    "dataset = read_csv(url, low_memory=False)\n",
    "\n",
    "names = list(dataset.columns)\n",
    "\n",
    "array = dataset.values\n",
    "f= array.shape[1]-1\n",
    "print('Number of instances:',array.shape[0])\n",
    "print('Number of features:',f)\n",
    "print('Labels:', set(array[:,f]))\n",
    "print('Each label count:')\n",
    "print(Counter(array[:,f]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94163a7",
   "metadata": {},
   "source": [
    "You can see that the dataset has over 1.5Mil data points (instances), with 23 features in each instance.\n",
    "\n",
    "The noticeable issue is that the dataset has about 1.2+ Mil class 0 (benign), and 200k class 1 (malicious) samples. This makes the dataset significantly imbalanced. We take care of that next, using random undersampling. The code below creates a balanced dataset where class 1 is 33% of the samples, and class 0 is 66% and store it in a file named dataset2.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter \n",
    "from tqdm import tqdm\n",
    "dataset = read_csv(\"dataset1.csv\")\n",
    "names = list(dataset.columns)\n",
    "\n",
    "array = dataset.values\n",
    "X = array[:,0:dataset.shape[1]-1]\n",
    "Y = array[:,dataset.shape[1]-1]\n",
    "\n",
    "class_names = set(Y)\n",
    "print('Before balancing:')\n",
    "print(Counter(Y))\n",
    "\n",
    "#This line was added because I received and error \"Unknown label type: %r\" when imblearn was handling\n",
    "# the balancing\n",
    "Y = Y.astype('int')\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.33)\n",
    "a, b = undersample.fit_resample(X, Y)\n",
    "\n",
    "print('After balancing:')\n",
    "print(Counter(b))\n",
    "array = np.c_[a,b]\n",
    "\n",
    "#saving the csv file\n",
    "dataset1 = DataFrame(array, columns = names)\n",
    "dataset1.to_csv('dataset2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d86f8",
   "metadata": {},
   "source": [
    "Now that we have a balanced and preprocessed dataset, we move to creating our ML classifier's pipeline.\n",
    "\n",
    "## Classifiers' training and testing\n",
    "\n",
    "The code shown next will create a piepline of five different types of classifiers; Random Forest, Logistic Regression, Decision Tree, Gaussian Naive Bayes, and Extreme Gradient Boost (XGB). The dataset will be randomly split into 75% training, and 25% testing samples with stratification.\n",
    "\n",
    "For each classifier, we will calculate the accuracy, precision, recall, and F<sub>1</sub> score. We will store all of the results in a file named `All-Algorithms-Results.csv`. We will also generate Confusion Matrix Plots for each of the classifiers to make an easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be55bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import xgboost\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load dataset\n",
    "url = \"dataset2.csv\"\n",
    "dataset = read_csv(url)\n",
    "\n",
    "names = list(dataset.columns)\n",
    "\n",
    "# Split-out training and testing datasets\n",
    "array = dataset.values\n",
    "X = array[:,0:dataset.shape[1]-1]\n",
    "Y = array[:,dataset.shape[1]-1]\n",
    "Y = Y.astype(int)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=1,stratify=Y)\n",
    "class_names = set(Y)\n",
    "\n",
    "# Preparing the output to be saved in a csv file for later reference\n",
    "tops = ['RF', 'LR','DT', 'GNB', 'XGB']\n",
    "allscores = np.empty([4,5], dtype=float)\n",
    "met = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "# Random Forest\n",
    "modelname='Random Forest'\n",
    "model1 = RandomForestClassifier()\n",
    "model1.fit(X_train, Y_train) # training\n",
    "predictions = model1.predict(X_test) # testing\n",
    "report = classification_report(Y_test, predictions, digits=4,output_dict=True)\n",
    "allscores[0,0] = report['accuracy']\n",
    "allscores[1,0] = report['macro avg']['precision']\n",
    "allscores[2,0] = report['macro avg']['recall']\n",
    "allscores[3,0] = report['macro avg']['f1-score']\n",
    "print(modelname,'\\n', classification_report(Y_test, predictions, digits=4))\n",
    "\n",
    "# Drawing the confusion matrix plot\n",
    "disp = ConfusionMatrixDisplay.from_estimator(model1, X_test, Y_test,display_labels=class_names,cmap=plt.cm.Blues,normalize='true',values_format='.4f')\n",
    "plt.gcf().set_dpi(200)\n",
    "disp.ax_.set_title(modelname)\n",
    "\n",
    "# Logistic Regression\n",
    "modelname='Logistic Regression'\n",
    "model2 = LogisticRegression(solver='liblinear')\n",
    "model2.fit(X_train, Y_train) # training\n",
    "predictions = model2.predict(X_test) # testing\n",
    "report = classification_report(Y_test, predictions, digits=4,output_dict=True)\n",
    "allscores[0,1] = report['accuracy']\n",
    "allscores[1,1] = report['macro avg']['precision']\n",
    "allscores[2,1] = report['macro avg']['recall']\n",
    "allscores[3,1] = report['macro avg']['f1-score']\n",
    "print(modelname,'\\n', classification_report(Y_test, predictions, digits=4))\n",
    "\n",
    "# Drawing the confusion matrix plot\n",
    "disp = ConfusionMatrixDisplay.from_estimator(model2, X_test, Y_test,display_labels=class_names,cmap=plt.cm.Blues,normalize='true',values_format='.4f')\n",
    "plt.gcf().set_dpi(200)\n",
    "disp.ax_.set_title(modelname)\n",
    "\n",
    "# Decision Tree\n",
    "modelname='Decision Tree'\n",
    "model3 = DecisionTreeClassifier()\n",
    "model3.fit(X_train, Y_train) # training\n",
    "predictions = model3.predict(X_test) # testing\n",
    "report = classification_report(Y_test, predictions, digits=4,output_dict=True)\n",
    "allscores[0,2] = report['accuracy']\n",
    "allscores[1,2] = report['macro avg']['precision']\n",
    "allscores[2,2] = report['macro avg']['recall']\n",
    "allscores[3,2] = report['macro avg']['f1-score']\n",
    "print(modelname,'\\n', classification_report(Y_test, predictions, digits=4))\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(model3, X_test, Y_test,display_labels=class_names,cmap=plt.cm.Blues,normalize='true',values_format='.4f')\n",
    "plt.gcf().set_dpi(200)\n",
    "disp.ax_.set_title(modelname)\n",
    "\n",
    "# GaussianNB\n",
    "modelname='GaussianNB'\n",
    "model4 = GaussianNB()\n",
    "model4.fit(X_train, Y_train) # training\n",
    "predictions = model4.predict(X_test) # testing\n",
    "report = classification_report(Y_test, predictions, digits=4,output_dict=True)\n",
    "allscores[0,3] = report['accuracy']\n",
    "allscores[1,3] = report['macro avg']['precision']\n",
    "allscores[2,3] = report['macro avg']['recall']\n",
    "allscores[3,3] = report['macro avg']['f1-score']\n",
    "print(modelname,'\\n', classification_report(Y_test, predictions, digits=4))\n",
    "\n",
    "# Drawing the confusion matrix plot\n",
    "disp = ConfusionMatrixDisplay.from_estimator(model4, X_test, Y_test,display_labels=class_names,cmap=plt.cm.Blues,normalize='true',values_format='.4f')\n",
    "plt.gcf().set_dpi(200)\n",
    "disp.ax_.set_title(modelname)\n",
    "\n",
    "# XGB\n",
    "modelname='XGB'\n",
    "model5 = xgboost.XGBClassifier(tree_method=\"hist\", device=\"cuda\")\n",
    "model5.fit(X_train, Y_train) # training\n",
    "predictions = model5.predict(X_test) # testing\n",
    "report = classification_report(Y_test, predictions, digits=4,output_dict=True)\n",
    "allscores[0,4] = report['accuracy']\n",
    "allscores[1,4] = report['macro avg']['precision']\n",
    "allscores[2,4] = report['macro avg']['recall']\n",
    "allscores[3,4] = report['macro avg']['f1-score']\n",
    "print(modelname,'\\n', classification_report(Y_test, predictions, digits=4))\n",
    "\n",
    "# Drawing the confusion matrix plot\n",
    "disp = ConfusionMatrixDisplay.from_estimator(model5, X_test, Y_test,display_labels=class_names,cmap=plt.cm.Blues,normalize='true',values_format='.4f')\n",
    "plt.gcf().set_dpi(200)\n",
    "disp.ax_.set_title(modelname)\n",
    "plt.show()\n",
    "\n",
    "# Storing the results in a csv file\n",
    "dataset1 = DataFrame(allscores, columns=tops)\n",
    "dataset1.insert(0,'Metric',met)\n",
    "dataset1 = dataset1.transpose()\n",
    "dataset1.to_csv('All-Algorithms-Results.csv',encoding='UTF_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a067c",
   "metadata": {},
   "source": [
    "Now you have your results, it's time to work on a few more things (on your own), such as cross-validation, validation using another dataset, explainability, hyperparameter optimization, deep neural networks, and many other things.\n",
    "\n",
    "You can find more details about the experiments and the results obtained in my paper **BotStop : Packet-based efficient and explainable IoT botnet detection using machine learning**. You can find it [here](https://doi.org/10.1016/j.comcom.2022.06.039).\n",
    "\n",
    "Thank you for sticking around!\n",
    "\n",
    "Sometimes, I have some interesting things to say. \n",
    "\n",
    "You can find my blog with some interesting how to's and tools on this link: <https://www.mohammedalani.com>\n",
    "\n",
    "My Youtube channel: <https://www.youtube.com/@DrMMAlani>\n",
    "\n",
    "And we can connect on LinkedIn: <https://ae.linkedin.com/in/prof-mohammed-m-alani>\n",
    "\n",
    "<img src=\"website.png\" alt=\"Link to MohammedAlani.com\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
